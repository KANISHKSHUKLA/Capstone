{
  "job_id": "1ac83730-62be-4bd4-b4b0-5c96e3da4db7",
  "created_at": "2025-11-20T00:09:52.466088",
  "status": "completed",
  "filename": "NIPS-2017-attention-is-all-you-need-Paper.pdf",
  "result": {
    "metadata": {
      "title": "Attention is All you Need",
      "authors": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin"
    },
    "key_concepts": {
      "key_concepts": [
        {
          "name": "Transformer",
          "category": "model architecture",
          "explanation": "A novel neural network architecture for sequence transduction that relies solely on attention mechanisms, entirely dispensing with recurrence and convolutions.",
          "relevance": "The central proposed model in the paper, designed to achieve superior quality, parallelizability, and significantly less training time for tasks like machine translation by processing input and output sequences in parallel rather than sequentially."
        },
        {
          "name": "Self-Attention (Intra-Attention)",
          "category": "attention mechanism",
          "explanation": "An attention mechanism that relates different positions of a single sequence to compute a representation of that sequence. It allows the model to weigh the importance of different parts of the input sequence when processing each part.",
          "relevance": "The fundamental building block of the Transformer, used in both the encoder and decoder to draw global dependencies within the input and output sequences, respectively."
        },
        {
          "name": "Multi-Head Attention",
          "category": "attention mechanism",
          "explanation": "An extension of self-attention where the queries, keys, and values are linearly projected multiple times with different learned projections. Each projection is then fed into a separate attention function (head) in parallel. The outputs of these heads are concatenated and linearly transformed to produce the final result.",
          "relevance": "Used in the Transformer to allow the model to jointly attend to information from different representation subspaces at different positions, counteracting the reduced effective resolution of single-head attention due to averaging."
        },
        {
          "name": "Scaled Dot-Product Attention",
          "category": "attention mechanism",
          "explanation": "A specific attention function where the output is computed as a weighted sum of values. The weights are obtained by taking the dot product of the query with all keys, dividing by the square root of the key dimension (dk), and then applying a softmax function.",
          "relevance": "The specific attention function used within each 'head' of the Multi-Head Attention mechanism in the Transformer. The scaling factor (1/√dk) is crucial for stable training with large dk values by preventing the softmax function from being pushed into regions with extremely small gradients."
        },
        {
          "name": "Encoder-Decoder Architecture",
          "category": "model architecture",
          "explanation": "A common structure in sequence transduction models where an encoder maps an input sequence of symbol representations to a sequence of continuous representations, and a decoder then generates an output sequence of symbols one element at a time, auto-regressively.",
          "relevance": "The Transformer follows this overall architecture, using stacked self-attention and point-wise fully connected layers for both the encoder and decoder."
        },
        {
          "name": "Positional Encoding",
          "category": "technique",
          "explanation": "Since the Transformer contains no recurrence and no convolution, it needs a mechanism to inject information about the relative or absolute position of tokens in the sequence. Positional encodings are added to the input embeddings to provide this sequential order information.",
          "relevance": "Essential for the Transformer to make use of the order of the sequence, as the attention mechanism itself is permutation-invariant."
        },
        {
          "name": "Residual Connections (Skip Connections)",
          "category": "technique",
          "explanation": "A technique where the output of a sub-layer is added to its input, allowing gradients to flow more easily through the network and helping to train deeper models.",
          "relevance": "Employed around each of the two sub-layers (multi-head self-attention and position-wise feed-forward network) in the encoder, and around each of the three sub-layers in the decoder, followed by layer normalization."
        },
        {
          "name": "Layer Normalization",
          "category": "technique",
          "explanation": "A normalization technique applied across the features of a single training example within a layer, helping to stabilize training and reduce training time.",
          "relevance": "Applied after the residual connection around each sub-layer in both the encoder and decoder of the Transformer."
        },
        {
          "name": "Position-wise Feed-Forward Networks",
          "category": "neural network component",
          "explanation": "A fully connected feed-forward network applied to each position separately and identically within the encoder and decoder layers. It consists of two linear transformations with a ReLU activation in between.",
          "relevance": "Forms the second sub-layer in each encoder and decoder layer, processing the output of the attention mechanism independently for each position."
        },
        {
          "name": "Masked Self-Attention (Decoder)",
          "category": "attention mechanism",
          "explanation": "A modification to the self-attention mechanism in the decoder that prevents positions from attending to subsequent positions. This is achieved by masking out (setting to -∞) values in the input of the softmax corresponding to illegal connections.",
          "relevance": "Ensures the auto-regressive property of the decoder, meaning predictions for position 'i' can only depend on known outputs at positions less than 'i'."
        }
      ],
      "core_technologies": [
        "Deep Learning Frameworks (implied for implementation)",
        "GPUs (specifically P100 GPUs mentioned for training acceleration)"
      ],
      "novelty_aspects": [
        "Introduction of the Transformer, the first transduction model relying entirely on self-attention to compute representations of its input and output, without using sequence-aligned Recurrent Neural Networks (RNNs) or convolutions.",
        "Achieving new state-of-the-art results in machine translation (28.4 BLEU on WMT 2014 English-to-German and 41.0 BLEU on WMT 2014 English-to-French) with significantly less training time and greater parallelization compared to previous recurrent or convolutional models.",
        "Dispensing with recurrence and convolutions entirely in the model architecture, relying solely on attention mechanisms."
      ],
      "field_of_study": "Natural Language Processing (NLP)",
      "interdisciplinary_connections": [
        "Machine Learning",
        "Deep Learning",
        "Artificial Intelligence"
      ]
    },
    "problem_statement": {
      "problem": "The dominant sequence transduction models, primarily based on complex recurrent or convolutional neural networks, face significant challenges in computational efficiency, parallelization, and effectively capturing long-range dependencies in tasks such as language modeling and machine translation. Existing attention mechanisms, while powerful, are typically used in conjunction with these inherently sequential or distance-limited architectures.",
      "research_questions": [
        "Can a novel network architecture, based solely on attention mechanisms and entirely dispensing with recurrence and convolutions, achieve state-of-the-art performance in sequence transduction tasks?",
        "Can such an attention-only model offer significant improvements in parallelization and reduce training time compared to existing recurrent and convolutional approaches?"
      ],
      "existing_approaches": [
        {
          "name": "Recurrent Neural Networks (RNNs), including Long Short-Term Memory (LSTMs) and Gated Recurrent Units (GRUs)",
          "limitations": [
            "Recurrent models typically factor computation along the symbol positions of the input and output sequences, which inherently precludes parallelization within training examples.",
            "This sequential nature becomes critical at longer sequence lengths, as memory constraints limit batching across examples.",
            "The fundamental constraint of sequential computation remains, even with improvements in computational efficiency through factorization tricks and conditional computation."
          ]
        },
        {
          "name": "Convolutional Neural Networks (CNNs) based models (e.g., Extended Neural GPU, ByteNet, ConvS2S)",
          "limitations": [
            "The number of operations required to relate signals from two arbitrary input or output positions grows with the distance between positions (linearly for ConvS2S and logarithmically for ByteNet).",
            "This increasing path length makes it more difficult to learn dependencies between distant positions."
          ]
        },
        {
          "name": "Encoder-decoder models with attention mechanisms (typically used in conjunction with RNNs)",
          "limitations": [
            "In all but a few cases, attention mechanisms are used in conjunction with a recurrent network. This implies that the full potential of attention for parallelization and direct global dependency modeling is not being independently leveraged."
          ]
        }
      ],
      "gap_in_research": "Existing state-of-the-art sequence transduction models are fundamentally constrained by sequential computation (RNNs) or struggle with long-range dependencies due to increasing path lengths (CNNs). While attention mechanisms have been integrated, they have not been explored as the *sole* computational primitive, leaving a gap for a model that leverages attention entirely to achieve full parallelization and constant path length for dependency modeling, thereby overcoming the limitations of prior architectures.",
      "importance": "Solving this problem is significant because it can lead to more computationally efficient and faster-training models for sequence transduction tasks like machine translation, while simultaneously achieving superior performance. A fully parallelizable architecture can better utilize modern hardware (like GPUs) and scale to longer sequences, advancing the state-of-the-art in natural language processing and other sequence-based domains. The proposed Transformer model demonstrates superior quality and significantly less training time, setting a new state of the art."
    },
    "full_explanation": {
      "title": "Attention Is All You Need",
      "authors": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin",
      "approach_summary": "The paper introduces the Transformer, a novel network architecture for sequence transduction that entirely foregoes recurrent and convolutional neural networks, relying solely on attention mechanisms. This approach aims to overcome the inherent sequential computation limitations of previous dominant models, enabling greater parallelization, reduced training times, and improved performance in tasks like machine translation.",
      "methodology": "The Transformer employs an encoder-decoder structure. The encoder processes the input sequence into a continuous representation, and the decoder generates the output sequence one element at a time, auto-regressively. Both the encoder and decoder are composed of stacks of identical layers. Each layer incorporates a multi-head self-attention mechanism and a position-wise fully connected feed-forward network. Residual connections are used around each sub-layer, followed by layer normalization. To inject information about the order of the sequence, as the model lacks recurrence or convolution, 'positional encodings' are added to the input embeddings.\n\nThe core attention function is 'Scaled Dot-Product Attention', which computes the output as a weighted sum of values, where weights are derived from the dot product of queries and keys, scaled by the square root of the key dimension (1/√dk) to prevent large magnitudes from pushing the softmax into regions with small gradients. 'Multi-Head Attention' enhances this by performing several attention functions in parallel on different linear projections of queries, keys, and values. The outputs from these 'heads' are then concatenated and linearly transformed, allowing the model to attend to information from different representation subspaces simultaneously.",
      "innovations": [
        "**The Transformer Architecture:** A groundbreaking model that entirely replaces recurrent and convolutional layers with attention mechanisms, marking a significant shift in sequence transduction modeling.",
        "**Scaled Dot-Product Attention:** An efficient attention function that includes a scaling factor (1/√dk) to stabilize gradients, particularly for large key dimensions, making the attention mechanism more robust.",
        "**Multi-Head Attention:** This mechanism allows the model to jointly attend to information from different representation subspaces at different positions, providing a richer and more diverse understanding of dependencies within the sequence.",
        "**Positional Encoding:** The introduction of learned positional encodings, added to input embeddings, to provide the model with information about the relative or absolute position of tokens, crucial in an architecture without inherent sequential processing.",
        "**Enhanced Parallelization and Training Efficiency:** The architecture's design inherently allows for significantly more parallelization during training compared to RNN-based models, leading to faster training times and reduced computational costs.",
        "**Constant Number of Operations for Long-Range Dependencies:** Unlike convolutional models where the number of operations to relate distant positions grows linearly or logarithmically, the Transformer reduces this to a constant number of operations."
      ],
      "architecture": "The Transformer follows an encoder-decoder structure, each composed of N=6 identical layers.\n\n**Encoder:** Each encoder layer consists of two sub-layers:\n1.  **Multi-Head Self-Attention Mechanism:** This sub-layer allows each position in the encoder to attend to all positions in the preceding layer of the encoder.\n2.  **Position-wise Fully Connected Feed-Forward Network:** This is a simple two-layer feed-forward network with a ReLU activation, applied identically and independently to each position.\nAround each of these two sub-layers, a residual connection is employed, followed by layer normalization. All sub-layers and the embedding layers produce outputs of dimension dmodel = 512.\n\n**Decoder:** Each decoder layer has three sub-layers:\n1.  **Masked Multi-Head Self-Attention Mechanism:** Similar to the encoder's self-attention, but modified to prevent positions from attending to subsequent positions. This masking ensures the auto-regressive property, where predictions for position 'i' can only depend on known outputs at positions less than 'i'.\n2.  **Multi-Head Encoder-Decoder Attention Mechanism:** In this sub-layer, the queries originate from the previous decoder layer, while the keys and values come from the output of the encoder stack. This enables every position in the decoder to attend over all positions in the input sequence.\n3.  **Position-wise Fully Connected Feed-Forward Network:** Identical in structure to the one in the encoder.\nSimilar to the encoder, residual connections are used around each sub-layer, followed by layer normalization.\n\n**Shared Components:**\n*   **Embeddings and Softmax:** Learned embeddings convert input and output tokens into dmodel-dimensional vectors. A learned linear transformation and softmax function convert the decoder output to predicted next-token probabilities. The same weight matrix is shared between the two embedding layers and the pre-softmax linear transformation, scaled by √dmodel.\n*   **Positional Encoding:** To provide sequence order information, positional encodings are added to the input embeddings at the bottom of both the encoder and decoder stacks. These encodings use sine and cosine functions of varying frequencies.",
      "evaluation": {
        "metrics": [
          "BLEU (Bilingual Evaluation Understudy) score"
        ],
        "datasets": [
          "WMT 2014 English-to-German translation task",
          "WMT 2014 English-to-French translation task"
        ],
        "baselines": [
          "Existing best results for machine translation, including ensemble models, which were primarily based on complex recurrent or convolutional neural networks."
        ]
      },
      "results": "The Transformer achieved state-of-the-art performance on both evaluated machine translation tasks, demonstrating superior quality and significant training efficiency:\n*   **WMT 2014 English-to-German Translation:** The model achieved a BLEU score of 28.4, which improved upon the existing best results, including ensembles, by over 2 BLEU points.\n*   **WMT 2014 English-to-French Translation:** The Transformer established a new single-model state-of-the-art BLEU score of 41.0 (or 41.8 in a later version of the paper). This was accomplished after training for only 3.5 days on eight GPUs, representing a small fraction of the training costs associated with the best models from the literature.\n\nThese results highlight the Transformer's ability to deliver higher translation quality while being significantly more parallelizable and requiring substantially less training time, making it a highly effective and scalable architecture for sequence transduction problems.",
      "limitations": [
        "The provided research paper content (abstract, introduction, and model architecture description) does not explicitly detail a dedicated 'Limitations' section. However, the paper implicitly addresses a potential limitation of single-head attention (reduced effective resolution due to averaging attention-weighted positions) by introducing Multi-Head Attention to counteract this effect.\n\nBroader discussions on Transformer limitations, not explicitly in the provided text but widely acknowledged in the field, include: computational complexity (quadratic with sequence length for self-attention), large memory requirements, and challenges with very long sequences."
      ],
      "future_work": [
        "The provided research paper content (abstract, introduction, and model architecture description) does not explicitly detail a 'Future Work' section. However, the paper notes that the Transformer generalizes well to other tasks, having been successfully applied to English constituency parsing, suggesting broader applicability.\n\nGeneral directions for future work on Transformers, as discussed in the broader research community, include: developing more efficient and smaller models, improving data-efficient learning methods (e.g., self-supervised and few-shot learning), enhancing cross-attention mechanisms for multimodal data, and exploring more sophisticated memory mechanisms for handling longer sequences."
      ]
    },
    "pseudo_code": {
      "implementation_overview": "This pseudo-code implements the Transformer model architecture as described in the paper 'Attention Is All You Need'. It focuses on the core components: Scaled Dot-Product Attention, Multi-Head Attention, Position-wise Feed-Forward Networks, Positional Encoding, and the stacked Encoder-Decoder structure with residual connections and layer normalization. The model eschews recurrence and convolutions, relying entirely on attention mechanisms for sequence transduction.",
      "prerequisites": [
        "Deep Learning Frameworks (e.g., TensorFlow, PyTorch) for tensor operations, automatic differentiation, and optimized linear algebra.",
        "NumPy or similar library for numerical operations.",
        "GPU acceleration (e.g., NVIDIA P100 GPUs mentioned in the paper) is highly recommended for practical training."
      ],
      "main_components": [
        "Scaled Dot-Product Attention",
        "Multi-Head Attention",
        "Position-wise Feed-Forward Network",
        "Positional Encoding",
        "Add & Norm (Residual Connection + Layer Normalization)",
        "Encoder Layer",
        "Decoder Layer",
        "Encoder Stack",
        "Decoder Stack",
        "Transformer Model"
      ],
      "pseudo_code": [
        {
          "component": "Scaled Dot-Product Attention",
          "description": "Computes attention scores by taking the dot product of queries with keys, scaling by the square root of the key dimension, applying a softmax, and then multiplying by values. Supports optional masking.",
          "code": "FUNCTION ScaledDotProductAttention(Q, K, V, mask=None):\n    // Q: Queries, K: Keys, V: Values\n    // mask: Optional mask (e.g., for padding or look-ahead)\n\n    dk = K.shape[-1] // Dimension of keys\n\n    // Compute attention scores: Q * K_transpose\n    scores = MATMUL(Q, K.TRANSPOSE(-2, -1)) / SQRT(dk)\n\n    // Apply mask if provided (e.g., for padding or preventing future information flow)\n    IF mask IS NOT None:\n        scores = scores + (mask * -1e9) // Set masked positions to a very large negative number\n\n    // Apply softmax to get attention weights\n    attention_weights = SOFTMAX(scores, axis=-1)\n\n    // Multiply weights with values to get the output\n    output = MATMUL(attention_weights, V)\n\n    RETURN output, attention_weights\n"
        },
        {
          "component": "Multi-Head Attention",
          "description": "Performs multiple Scaled Dot-Product Attention operations in parallel on different linear projections of queries, keys, and values. The outputs are concatenated and then linearly projected back to the original dimension.",
          "code": "FUNCTION MultiHeadAttention(Q, K, V, d_model, num_heads, mask=None):\n    // Q, K, V: Input tensors for queries, keys, values\n    // d_model: Dimensionality of the model (e.g., 512)\n    // num_heads: Number of attention heads (e.g., 8)\n    // mask: Optional mask\n\n    dk = d_model / num_heads // Dimension for each head's keys\n    dv = d_model / num_heads // Dimension for each head's values\n\n    // Learned linear projection matrices for Q, K, V for each head\n    // W_Q[h], W_K[h], W_V[h] are of shape (d_model, dk) or (d_model, dv)\n    // W_O is of shape (num_heads * dv, d_model)\n    DECLARE W_Q[num_heads], W_K[num_heads], W_V[num_heads], W_O AS LEARNABLE_PARAMETERS\n\n    heads_outputs = []\n\n    FOR h FROM 0 TO num_heads - 1:\n        // Linearly project Q, K, V for the current head\n        Q_h = LINEAR_TRANSFORM(Q, W_Q[h]) // Q_h shape: (batch_size, seq_len, dk)\n        K_h = LINEAR_TRANSFORM(K, W_K[h]) // K_h shape: (batch_size, seq_len, dk)\n        V_h = LINEAR_TRANSFORM(V, W_V[h]) // V_h shape: (batch_size, seq_len, dv)\n\n        // Perform scaled dot-product attention for the current head\n        head_output, _ = ScaledDotProductAttention(Q_h, K_h, V_h, mask)\n        heads_outputs.APPEND(head_output)\n\n    // Concatenate all head outputs along the last dimension\n    concat_heads = CONCATENATE(heads_outputs, axis=-1) // Shape: (batch_size, seq_len, num_heads * dv)\n\n    // Linearly project the concatenated output back to d_model\n    output = LINEAR_TRANSFORM(concat_heads, W_O) // Shape: (batch_size, seq_len, d_model)\n\n    RETURN output\n"
        },
        {
          "component": "Position-wise Feed-Forward Network",
          "description": "A simple feed-forward network applied independently to each position. It consists of two linear transformations with a ReLU activation in between.",
          "code": "FUNCTION PositionwiseFeedForward(x, d_model, d_ff):\n    // x: Input tensor (batch_size, seq_len, d_model)\n    // d_model: Input/output dimensionality (e.g., 512)\n    // d_ff: Inner-layer dimensionality (e.g., 2048)\n\n    // W1, b1: Parameters for the first linear transformation (d_model -> d_ff)\n    // W2, b2: Parameters for the second linear transformation (d_ff -> d_model)\n    DECLARE W1, b1, W2, b2 AS LEARNABLE_PARAMETERS\n\n    // First linear transformation + ReLU activation\n    output = LINEAR_TRANSFORM(x, W1) + b1\n    output = RELU(output)\n\n    // Second linear transformation\n    output = LINEAR_TRANSFORM(output, W2) + b2\n\n    RETURN output\n"
        },
        {
          "component": "Positional Encoding",
          "description": "Injects information about the relative or absolute position of tokens into the input embeddings, as the Transformer contains no recurrence or convolution.",
          "code": "FUNCTION PositionalEncoding(max_seq_len, d_model):\n    // max_seq_len: Maximum sequence length the model can handle\n    // d_model: Dimensionality of the model\n\n    pe = ZEROS(max_seq_len, d_model) // Initialize positional encoding matrix\n\n    FOR pos FROM 0 TO max_seq_len - 1:\n        FOR i FROM 0 TO d_model / 2 - 1: // Iterate for sin and cos pairs\n            denominator = 10000^(2 * i / d_model)\n            pe[pos, 2 * i] = SIN(pos / denominator)\n            pe[pos, 2 * i + 1] = COS(pos / denominator)\n\n    RETURN pe // Shape: (max_seq_len, d_model)\n"
        },
        {
          "component": "Add & Norm (Residual Connection + Layer Normalization)",
          "description": "Applies a residual connection followed by layer normalization. Output = LayerNorm(x + Sublayer(x)).",
          "code": "FUNCTION AddAndNorm(x, sublayer_output):\n    // x: Input to the sub-layer\n    // sublayer_output: Output of the sub-layer (e.g., attention or FFN)\n\n    // Residual connection\n    summed_output = x + sublayer_output\n\n    // Layer Normalization\n    normalized_output = LAYER_NORM(summed_output)\n\n    RETURN normalized_output\n"
        },
        {
          "component": "Encoder Layer",
          "description": "Each encoder layer consists of a Multi-Head Self-Attention mechanism and a Position-wise Feed-Forward Network, each followed by a residual connection and layer normalization.",
          "code": "FUNCTION EncoderLayer(x, d_model, num_heads, d_ff, padding_mask):\n    // x: Input to the encoder layer (batch_size, seq_len, d_model)\n    // d_model, num_heads, d_ff: Model hyperparameters\n    // padding_mask: Mask to ignore padding tokens in self-attention\n\n    // Sub-layer 1: Multi-Head Self-Attention\n    // Q, K, V all come from the same input 'x'\n    self_attn_output = MultiHeadAttention(x, x, x, d_model, num_heads, padding_mask)\n    x = AddAndNorm(x, self_attn_output)\n\n    // Sub-layer 2: Position-wise Feed-Forward Network\n    ffn_output = PositionwiseFeedForward(x, d_model, d_ff)\n    x = AddAndNorm(x, ffn_output)\n\n    RETURN x\n"
        },
        {
          "component": "Decoder Layer",
          "description": "Each decoder layer has three sub-layers: masked Multi-Head Self-Attention, Multi-Head Encoder-Decoder Attention, and a Position-wise Feed-Forward Network. All are followed by residual connections and layer normalization.",
          "code": "FUNCTION DecoderLayer(x, enc_output, d_model, num_heads, d_ff, look_ahead_mask, padding_mask):\n    // x: Input to the decoder layer (batch_size, seq_len, d_model)\n    // enc_output: Output from the encoder stack (batch_size, enc_seq_len, d_model)\n    // d_model, num_heads, d_ff: Model hyperparameters\n    // look_ahead_mask: Mask to prevent attending to future positions in decoder self-attention\n    // padding_mask: Mask for encoder output padding in encoder-decoder attention\n\n    // Sub-layer 1: Masked Multi-Head Self-Attention\n    // Q, K, V all come from 'x', with a look-ahead mask\n    masked_self_attn_output = MultiHeadAttention(x, x, x, d_model, num_heads, look_ahead_mask)\n    x = AddAndNorm(x, masked_self_attn_output)\n\n    // Sub-layer 2: Multi-Head Encoder-Decoder Attention\n    // Queries come from 'x', Keys and Values come from 'enc_output'\n    enc_dec_attn_output = MultiHeadAttention(x, enc_output, enc_output, d_model, num_heads, padding_mask)\n    x = AddAndNorm(x, enc_dec_attn_output)\n\n    // Sub-layer 3: Position-wise Feed-Forward Network\n    ffn_output = PositionwiseFeedForward(x, d_model, d_ff)\n    x = AddAndNorm(x, ffn_output)\n\n    RETURN x\n"
        },
        {
          "component": "Encoder Stack",
          "description": "Composed of N identical Encoder Layers. Takes input embeddings and positional encodings, and processes them through the stack.",
          "code": "FUNCTION Encoder(input_tokens, N_layers, d_model, num_heads, d_ff, src_vocab_size, max_seq_len):\n    // input_tokens: Input sequence of token IDs (batch_size, seq_len)\n    // N_layers: Number of encoder layers (e.g., 6)\n    // Other parameters: Model hyperparameters and vocabulary size\n\n    // 1. Input Embeddings\n    x = LEARNED_EMBEDDING(input_tokens, src_vocab_size, d_model) // Shape: (batch_size, seq_len, d_model)\n\n    // 2. Positional Encoding\n    pos_encoding = PositionalEncoding(max_seq_len, d_model)\n    x = x + pos_encoding[:x.shape[1], :] // Add positional encodings to embeddings\n\n    // 3. Create padding mask for self-attention\n    padding_mask = CREATE_PADDING_MASK(input_tokens) // Mask for source sequence padding\n\n    // 4. Stack N Encoder Layers\n    FOR layer_idx FROM 0 TO N_layers - 1:\n        x = EncoderLayer(x, d_model, num_heads, d_ff, padding_mask)\n\n    RETURN x // Final encoder output (batch_size, seq_len, d_model)\n"
        },
        {
          "component": "Decoder Stack",
          "description": "Composed of N identical Decoder Layers. Takes target embeddings, positional encodings, and the encoder output, processing them through the stack.",
          "code": "FUNCTION Decoder(target_tokens, enc_output, N_layers, d_model, num_heads, d_ff, tgt_vocab_size, max_seq_len):\n    // target_tokens: Target sequence of token IDs (batch_size, seq_len)\n    // enc_output: Output from the Encoder stack\n    // N_layers: Number of decoder layers (e.g., 6)\n    // Other parameters: Model hyperparameters and vocabulary size\n\n    // 1. Target Embeddings\n    x = LEARNED_EMBEDDING(target_tokens, tgt_vocab_size, d_model) // Shape: (batch_size, seq_len, d_model)\n\n    // 2. Positional Encoding\n    pos_encoding = PositionalEncoding(max_seq_len, d_model)\n    x = x + pos_encoding[:x.shape[1], :] // Add positional encodings to embeddings\n\n    // 3. Create masks for decoder attention\n    look_ahead_mask = CREATE_LOOK_AHEAD_MASK(target_tokens.shape[1]) // Mask for decoder self-attention\n    // Padding mask for encoder output in encoder-decoder attention (based on source input)\n    // This mask needs to be derived from the *source* input's padding, not target_tokens\n    // For simplicity, assume enc_output_padding_mask is passed or derived from source_input_tokens\n    enc_output_padding_mask = CREATE_PADDING_MASK_FOR_ENCODER_OUTPUT(enc_output) \n\n    // 4. Stack N Decoder Layers\n    FOR layer_idx FROM 0 TO N_layers - 1:\n        x = DecoderLayer(x, enc_output, d_model, num_heads, d_ff, look_ahead_mask, enc_output_padding_mask)\n\n    RETURN x // Final decoder output (batch_size, seq_len, d_model)\n"
        },
        {
          "component": "Transformer Model",
          "description": "The complete Transformer model, combining the Encoder and Decoder stacks with input/output embeddings and a final linear-softmax layer for token prediction.",
          "code": "FUNCTION Transformer(src_vocab_size, tgt_vocab_size, N_layers, d_model, num_heads, d_ff, max_seq_len):\n    // src_vocab_size: Size of the source language vocabulary\n    // tgt_vocab_size: Size of the target language vocabulary\n    // N_layers: Number of encoder/decoder layers (e.g., 6)\n    // d_model: Model dimensionality (e.g., 512)\n    // num_heads: Number of attention heads (e.g., 8)\n    // d_ff: Feed-forward inner dimension (e.g., 2048)\n    // max_seq_len: Maximum sequence length\n\n    // Initialize Encoder and Decoder stacks\n    encoder_stack = Encoder(N_layers, d_model, num_heads, d_ff, src_vocab_size, max_seq_len)\n    decoder_stack = Decoder(N_layers, d_model, num_heads, d_ff, tgt_vocab_size, max_seq_len)\n\n    // Final linear layer for output token prediction\n    final_linear_layer = LINEAR_TRANSFORM_LAYER(d_model, tgt_vocab_size)\n\n    FUNCTION FORWARD(source_input_tokens, target_input_tokens):\n        // source_input_tokens: Batch of source sequences (e.g., English sentence IDs)\n        // target_input_tokens: Batch of target sequences (e.g., partially generated French sentence IDs)\n\n        // 1. Encoder forward pass\n        // The padding mask for the encoder is created based on source_input_tokens\n        src_padding_mask = CREATE_PADDING_MASK(source_input_tokens)\n        enc_output = encoder_stack.FORWARD(source_input_tokens, src_padding_mask)\n\n        // 2. Decoder forward pass\n        // The look-ahead mask is created based on target_input_tokens length\n        // The encoder-decoder attention mask is the same as src_padding_mask\n        tgt_look_ahead_mask = CREATE_LOOK_AHEAD_MASK(target_input_tokens.shape[1])\n        dec_output = decoder_stack.FORWARD(target_input_tokens, enc_output, tgt_look_ahead_mask, src_padding_mask)\n\n        // 3. Final linear projection to vocabulary size\n        logits = final_linear_layer.FORWARD(dec_output)\n\n        // 4. Softmax to get probabilities (often done in loss function for numerical stability)\n        probabilities = SOFTMAX(logits, axis=-1)\n\n        RETURN probabilities\n\n    RETURN FORWARD\n\n// Helper functions (conceptual, actual implementation depends on framework):\nFUNCTION CREATE_PADDING_MASK(seq_tokens):\n    // Returns a mask where 1 indicates padding (to be masked out), 0 otherwise.\n    // Example: mask = (seq_tokens == PAD_TOKEN_ID).unsqueeze(1).unsqueeze(1)\n    // This mask needs to be broadcastable to attention scores (batch, 1, 1, seq_len) or (batch, 1, seq_len, seq_len)\n    // For encoder self-attention, it's (batch, 1, 1, seq_len)\n    // For encoder-decoder attention, it's (batch, 1, 1, enc_seq_len)\n    RETURN mask\n\nFUNCTION CREATE_LOOK_AHEAD_MASK(seq_len):\n    // Returns a square mask (seq_len, seq_len) where upper triangle is 1 (to be masked out)\n    // Example: mask = TRIU(ONES(seq_len, seq_len), diagonal=1)\n    // Combine with padding mask if target_tokens can have padding\n    RETURN mask\n\nFUNCTION LEARNED_EMBEDDING(tokens, vocab_size, d_model):\n    // Standard embedding layer lookup\n    RETURN EMBEDDING_LOOKUP(tokens, vocab_size, d_model)\n\nFUNCTION LINEAR_TRANSFORM(input_tensor, weight_matrix, bias_vector=None):\n    // Standard matrix multiplication and optional bias addition\n    RETURN MATMUL(input_tensor, weight_matrix) + bias_vector (if bias_vector exists)\n\nFUNCTION LAYER_NORM(input_tensor):\n    // Standard layer normalization operation\n    RETURN NORMALIZE(input_tensor)\n\nFUNCTION RELU(input_tensor):\n    // Rectified Linear Unit activation\n    RETURN MAX(0, input_tensor)\n\nFUNCTION SOFTMAX(input_tensor, axis):\n    // Softmax activation function\n    RETURN EXP(input_tensor) / SUM(EXP(input_tensor), axis=axis)\n\nFUNCTION CONCATENATE(list_of_tensors, axis):\n    // Concatenates tensors along a specified axis\n    RETURN CONCAT(list_of_tensors, axis)\n\nFUNCTION ZEROS(shape):\n    // Creates a tensor filled with zeros\n    RETURN TENSOR_OF_ZEROS(shape)\n\nFUNCTION SIN(value):\n    // Sine function\n    RETURN MATH.SIN(value)\n\nFUNCTION COS(value):\n    // Cosine function\n    RETURN MATH.COS(value)\n\nFUNCTION SQRT(value):\n    // Square root function\n    RETURN MATH.SQRT(value)\n"
        }
      ],
      "usage_example": "```python\n# Conceptual Python-like usage example\n\n# Define hyperparameters based on the paper\nd_model = 512\nnum_heads = 8\nd_ff = 2048\nN_layers = 6\nsrc_vocab_size = 10000  # Example source vocabulary size\ntgt_vocab_size = 12000  # Example target vocabulary size\nmax_seq_len = 200     # Example maximum sequence length\nPAD_TOKEN_ID = 0      # Example padding token ID\n\n# Instantiate the Transformer model\ntransformer_model = Transformer(\n    src_vocab_size=src_vocab_size,\n    tgt_vocab_size=tgt_vocab_size,\n    N_layers=N_layers,\n    d_model=d_model,\n    num_heads=num_heads,\n    d_ff=d_ff,\n    max_seq_len=max_seq_len\n)\n\n# Example input data (batch_size=2, seq_len=10 for source, 8 for target)\nsource_input_batch = [\n    [10, 20, 30, 40, 50, PAD_TOKEN_ID, PAD_TOKEN_ID, PAD_TOKEN_ID, PAD_TOKEN_ID, PAD_TOKEN_ID],\n   \n] # Example token IDs, padded\ntarget_input_batch = [\n    [1, 2, 3, 4, 5, PAD_TOKEN_ID, PAD_TOKEN_ID, PAD_TOKEN_ID],\n   \n] # Example token IDs, padded\n\n# Convert to tensors (framework-specific, e.g., PyTorch tensors or TensorFlow Tensors)\nsource_input_tensor = convert_to_tensor(source_input_batch, dtype=int)\ntarget_input_tensor = convert_to_tensor(target_input_batch, dtype=int)\n\n# Forward pass through the model\n# In a real scenario, target_input_tensor would be shifted right (start with <SOS> token)\n# and the last token would be masked for loss calculation.\noutput_probabilities = transformer_model.FORWARD(source_input_tensor, target_input_tensor)\n\nprint(\"Output probabilities shape:\", output_probabilities.shape)\n# Expected shape: (batch_size, target_seq_len, tgt_vocab_size)\n# e.g., (2, 8, 12000)\n\n# During training, a loss function (e.g., CrossEntropyLoss) would be applied\n# to output_probabilities and the actual next tokens.\n\n# For inference (decoding), an auto-regressive process would be used:\n# 1. Start with <SOS> token for target_input_tensor.\n# 2. Predict next token.\n# 3. Append predicted token to target_input_tensor.\n# 4. Repeat until <EOS> token is predicted or max_seq_len is reached.\n```",
      "potential_challenges": [
        "**Memory Consumption:** Multi-Head Attention, especially with large sequence lengths, can be memory-intensive due to the quadratic complexity of attention scores (Q K^T). Batching and sequence length management are crucial.",
        "**Masking Implementation:** Correctly implementing padding masks for both encoder and decoder, and the look-ahead mask for decoder self-attention, is critical and a common source of errors. Masks must be broadcastable to the attention score matrices.",
        "**Positional Encoding:** While conceptually simple, ensuring correct calculation and addition of positional encodings, especially for variable sequence lengths, requires careful indexing.",
        "**Hyperparameter Tuning:** Optimal performance heavily depends on hyperparameters like `d_model`, `num_heads`, `d_ff`, learning rate schedules, and dropout rates.",
        "**Training Stability:** Transformers can sometimes be sensitive to initialization and learning rate schedules. Techniques like warm-up learning rates are often necessary.",
        "**Debugging:** The layered structure with residual connections and normalization can make debugging challenging. Intermediate tensor shapes and values should be monitored.",
        "**Computational Graph Complexity:** Building the full computational graph for the Transformer, especially with dynamic masking, requires a robust deep learning framework."
      ]
    },
    "knowledge_graph": {
      "nodes": [],
      "edges": []
    },
    "architecture_deep_dive": {},
    "model_file": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport math\n\nclass PositionalEncoding(nn.Module):\n    \"\"\"\n    Implements the positional encoding as described in the paper \"Attention Is All You Need\".\n    This adds information about the relative or absolute position of the tokens in the sequence.\n    The encodings are added to the input embeddings at the bottom of the encoder and decoder stacks.\n    \"\"\"\n    def __init__(self, d_model: int, max_len: int = 5000):\n        \"\"\"\n        Initializes the PositionalEncoding layer.\n\n        Args:\n            d_model (int): The dimension of the model's embeddings (e.g., 512).\n            max_len (int): The maximum sequence length for which to generate positional encodings.\n                           This determines the size of the pre-computed positional encoding matrix.\n        \"\"\"\n        super(PositionalEncoding, self).__init__()\n        pe = torch.zeros(max_len, d_model) # [max_len, d_model]\n        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1) # [max_len, 1]\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)) # [d_model/2]\n        pe[:, 0::2] = torch.sin(position * div_term) # [max_len, d_model/2]\n        pe[:, 1::2] = torch.cos(position * div_term) # [max_len, d_model/2]\n        self.register_buffer('pe', pe.unsqueeze(0)) # [1, max_len, d_model]\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Adds positional encoding to the input tensor.\n\n        Args:\n            x (torch.Tensor): The input tensor (e.g., word embeddings).\n                              Expected shape: [batch_size, seq_len, d_model].\n\n        Returns:\n            torch.Tensor: The input tensor with positional encoding added.\n                          Shape: [batch_size, seq_len, d_model].\n        \"\"\"\n        # [batch_size, seq_len, d_model] + [1, seq_len, d_model]\n        # Example: [1, 10, 512] + [1, 10, 512]\n        x = x + self.pe[:, :x.size(1)]\n        return x\n\nclass ScaledDotProductAttention(nn.Module):\n    \"\"\"\n    Implements the Scaled Dot-Product Attention mechanism.\n    Attention(Q, K, V) = softmax(Q K^T / sqrt(d_k)) V\n    \"\"\"\n    def __init__(self, dropout_rate: float = 0.1):\n        \"\"\"\n        Initializes the ScaledDotProductAttention layer.\n\n        Args:\n            dropout_rate (float): The dropout probability to apply to attention weights.\n        \"\"\"\n        super(ScaledDotProductAttention, self).__init__()\n        self.dropout = nn.Dropout(dropout_rate)\n\n    def forward(self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, mask: torch.Tensor = None) -> torch.Tensor:\n        \"\"\"\n        Computes the scaled dot-product attention.\n\n        Args:\n            q (torch.Tensor): Queries tensor. Shape: [batch_size, num_heads, seq_len_q, d_k].\n            k (torch.Tensor): Keys tensor. Shape: [batch_size, num_heads, seq_len_k, d_k].\n            v (torch.Tensor): Values tensor. Shape: [batch_size, num_heads, seq_len_v, d_v].\n                              Note: seq_len_k must be equal to seq_len_v.\n            mask (torch.Tensor, optional): An attention mask. Shape: [batch_size, 1, 1, seq_len_k]\n                                           or [batch_size, 1, seq_len_q, seq_len_k].\n                                           Masked positions are set to a very small negative value (-1e9).\n                                           Defaults to None.\n\n        Returns:\n            torch.Tensor: The output of the attention mechanism.\n                          Shape: [batch_size, num_heads, seq_len_q, d_v].\n        \"\"\"\n        d_k = q.size(-1)\n        # [batch_size, num_heads, seq_len_q, d_k] @ [batch_size, num_heads, d_k, seq_len_k]\n        # -> [batch_size, num_heads, seq_len_q, seq_len_k]\n        # Example: [1, 8, 10, 64] @ [1, 8, 64, 10] -> [1, 8, 10, 10]\n        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(d_k)\n\n        if mask is not None:\n            # Apply mask by setting masked positions to a very small negative number\n            # [batch_size, num_heads, seq_len_q, seq_len_k] + [batch_size, 1, 1, seq_len_k] (broadcast)\n            # Example: [1, 8, 10, 10] + [1, 1, 1, 10]\n            scores = scores.masked_fill(mask == 0, -1e9)\n\n        # [batch_size, num_heads, seq_len_q, seq_len_k]\n        # Example: [1, 8, 10, 10]\n        attention_weights = F.softmax(scores, dim=-1)\n        attention_weights = self.dropout(attention_weights)\n\n        # [batch_size, num_heads, seq_len_q, seq_len_k] @ [batch_size, num_heads, seq_len_v, d_v]\n        # -> [batch_size, num_heads, seq_len_q, d_v]\n        # Example: [1, 8, 10, 10] @ [1, 8, 10, 64] -> [1, 8, 10, 64]\n        output = torch.matmul(attention_weights, v)\n        return output\n\nclass MultiHeadAttention(nn.Module):\n    \"\"\"\n    Implements the Multi-Head Attention mechanism.\n    This allows the model to jointly attend to information from different\n    representation subspaces at different positions.\n    \"\"\"\n    def __init__(self, d_model: int, num_heads: int, dropout_rate: float = 0.1):\n        \"\"\"\n        Initializes the MultiHeadAttention layer.\n\n        Args:\n            d_model (int): The dimension of the model's embeddings (e.g., 512).\n            num_heads (int): The number of attention heads (e.g., 8).\n            dropout_rate (float): The dropout probability.\n        \"\"\"\n        super(MultiHeadAttention, self).__init__()\n        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n\n        self.d_k = d_model // num_heads # Dimension of keys/queries per head\n        self.d_v = d_model // num_heads # Dimension of values per head\n        self.num_heads = num_heads\n        self.d_model = d_model\n\n        self.w_q = nn.Linear(d_model, d_model) # Linear projection for queries\n        self.w_k = nn.Linear(d_model, d_model) # Linear projection for keys\n        self.w_v = nn.Linear(d_model, d_model) # Linear projection for values\n        self.fc_out = nn.Linear(d_model, d_model) # Final linear projection\n\n        self.attention = ScaledDotProductAttention(dropout_rate)\n        self.dropout = nn.Dropout(dropout_rate)\n\n    def forward(self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, mask: torch.Tensor = None) -> torch.Tensor:\n        \"\"\"\n        Computes the multi-head attention.\n\n        Args:\n            q (torch.Tensor): Queries tensor. Shape: [batch_size, seq_len_q, d_model].\n            k (torch.Tensor): Keys tensor. Shape: [batch_size, seq_len_k, d_model].\n            v (torch.Tensor): Values tensor. Shape: [batch_size, seq_len_v, d_model].\n            mask (torch.Tensor, optional): An attention mask. Shape: [batch_size, 1, 1, seq_len_k]\n                                           or [batch_size, 1, seq_len_q, seq_len_k].\n                                           Defaults to None.\n\n        Returns:\n            torch.Tensor: The output of the multi-head attention mechanism.\n                          Shape: [batch_size, seq_len_q, d_model].\n        \"\"\"\n        batch_size = q.size(0)\n\n        # 1) Linear projections and reshape for multi-head attention\n        # [batch_size, seq_len, d_model] -> [batch_size, seq_len, d_model]\n        # -> [batch_size, seq_len, num_heads, d_k/d_v]\n        # -> [batch_size, num_heads, seq_len, d_k/d_v]\n        # Example: [1, 10, 512] -> [1, 10, 512] -> [1, 10, 8, 64] -> [1, 8, 10, 64]\n        q = self.w_q(q).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n        k = self.w_k(k).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n        v = self.w_v(v).view(batch_size, -1, self.num_heads, self.d_v).transpose(1, 2)\n\n        # 2) Apply scaled dot-product attention\n        # Output shape: [batch_size, num_heads, seq_len_q, d_v]\n        # Example: [1, 8, 10, 64]\n        attention_output = self.attention(q, k, v, mask)\n\n        # 3) Concatenate heads and apply final linear projection\n        # [batch_size, num_heads, seq_len_q, d_v]\n        # -> [batch_size, seq_len_q, num_heads, d_v] (undo transpose)\n        # -> [batch_size, seq_len_q, d_model] (concatenate heads)\n        # Example: [1, 8, 10, 64] -> [1, 10, 8, 64] -> [1, 10, 512]\n        attention_output = attention_output.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n\n        # [batch_size, seq_len_q, d_model] -> [batch_size, seq_len_q, d_model]\n        # Example: [1, 10, 512] -> [1, 10, 512]\n        output = self.fc_out(attention_output)\n        return self.dropout(output)\n\nclass PositionwiseFeedForward(nn.Module):\n    \"\"\"\n    Implements the Position-wise Fully Connected Feed-Forward Network.\n    This is a simple two-layer feed-forward network with a ReLU activation,\n    applied identically and independently to each position.\n    FFN(x) = max(0, x W1 + b1) W2 + b2\n    \"\"\"\n    def __init__(self, d_model: int, d_ff: int, dropout_rate: float = 0.1):\n        \"\"\"\n        Initializes the PositionwiseFeedForward layer.\n\n        Args:\n            d_model (int): The dimension of the model's embeddings (e.g., 512).\n            d_ff (int): The dimension of the inner-layer (e.g., 2048).\n            dropout_rate (float): The dropout probability.\n        \"\"\"\n        super(PositionwiseFeedForward, self).__init__()\n        self.w_1 = nn.Linear(d_model, d_ff)\n        self.w_2 = nn.Linear(d_ff, d_model)\n        self.dropout = nn.Dropout(dropout_rate)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Applies the position-wise feed-forward network.\n\n        Args:\n            x (torch.Tensor): Input tensor. Shape: [batch_size, seq_len, d_model].\n                              Example: [1, 10, 512].\n\n        Returns:\n            torch.Tensor: Output tensor. Shape: [batch_size, seq_len, d_model].\n                          Example: [1, 10, 512].\n        \"\"\"\n        # [batch_size, seq_len, d_model] -> [batch_size, seq_len, d_ff]\n        # Example: [1, 10, 512] -> [1, 10, 2048]\n        x = F.relu(self.w_1(x))\n        x = self.dropout(x)\n        # [batch_size, seq_len, d_ff] -> [batch_size, seq_len, d_model]\n        # Example: [1, 10, 2048] -> [1, 10, 512]\n        return self.w_2(x)\n\nclass EncoderLayer(nn.Module):\n    \"\"\"\n    Implements a single Encoder Layer of the Transformer.\n    Each layer consists of two sub-layers:\n    1. Multi-Head Self-Attention Mechanism.\n    2. Position-wise Fully Connected Feed-Forward Network.\n    Residual connections are employed around each sub-layer, followed by layer normalization.\n    \"\"\"\n    def __init__(self, d_model: int, num_heads: int, d_ff: int, dropout_rate: float = 0.1):\n        \"\"\"\n        Initializes an EncoderLayer.\n\n        Args:\n            d_model (int): The dimension of the model's embeddings (e.g., 512).\n            num_heads (int): The number of attention heads (e.g., 8).\n            d_ff (int): The dimension of the inner-layer of the feed-forward network (e.g., 2048).\n            dropout_rate (float): The dropout probability.\n        \"\"\"\n        super(EncoderLayer, self).__init__()\n        self.self_attn = MultiHeadAttention(d_model, num_heads, dropout_rate)\n        self.feed_forward = PositionwiseFeedForward(d_model, d_ff, dropout_rate)\n        self.norm1 = nn.LayerNorm(d_model)\n        self.norm2 = nn.LayerNorm(d_model)\n        self.dropout = nn.Dropout(dropout_rate)\n\n    def forward(self, x: torch.Tensor, mask: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs a forward pass through an EncoderLayer.\n\n        Args:\n            x (torch.Tensor): Input tensor from the previous layer.\n                              Shape: [batch_size, seq_len, d_model].\n                              Example: [1, 10, 512].\n            mask (torch.Tensor): Source mask for self-attention.\n                                 Shape: [batch_size, 1, 1, seq_len_src].\n                                 Example: [1, 1, 1, 10].\n\n        Returns:\n            torch.Tensor: Output tensor of the encoder layer.\n                          Shape: [batch_size, seq_len, d_model].\n                          Example: [1, 10, 512].\n        \"\"\"\n        # Sub-layer 1: Multi-Head Self-Attention\n        # [batch_size, seq_len, d_model] -> [batch_size, seq_len, d_model]\n        # Example: [1, 10, 512] -> [1, 10, 512]\n        attn_output = self.self_attn(x, x, x, mask)\n        # Add residual connection and apply layer normalization\n        # [batch_size, seq_len, d_model] + [batch_size, seq_len, d_model]\n        # Example: [1, 10, 512] + [1, 10, 512]\n        x = self.norm1(x + self.dropout(attn_output))\n\n        # Sub-layer 2: Position-wise Feed-Forward\n        # [batch_size, seq_len, d_model] -> [batch_size, seq_len, d_model]\n        # Example: [1, 10, 512] -> [1, 10, 512]\n        ff_output = self.feed_forward(x)\n        # Add residual connection and apply layer normalization\n        # [batch_size, seq_len, d_model] + [batch_size, seq_len, d_model]\n        # Example: [1, 10, 512] + [1, 10, 512]\n        x = self.norm2(x + self.dropout(ff_output))\n        return x\n\nclass Encoder(nn.Module):\n    \"\"\"\n    Implements the Encoder stack of the Transformer.\n    Composed of N identical EncoderLayers.\n    Input embeddings are combined with positional encodings.\n    \"\"\"\n    def __init__(self, input_vocab_size: int, d_model: int, num_layers: int,\n                 num_heads: int, d_ff: int, max_seq_len: int, dropout_rate: float = 0.1):\n        \"\"\"\n        Initializes the Encoder.\n\n        Args:\n            input_vocab_size (int): The size of the input vocabulary.\n            d_model (int): The dimension of the model's embeddings (e.g., 512).\n            num_layers (int): The number of identical encoder layers (N, e.g., 6).\n            num_heads (int): The number of attention heads (e.g., 8).\n            d_ff (int): The dimension of the inner-layer of the feed-forward network (e.g., 2048).\n            max_seq_len (int): The maximum sequence length for positional encoding.\n            dropout_rate (float): The dropout probability.\n        \"\"\"\n        super(Encoder, self).__init__()\n        self.d_model = d_model\n        self.embedding = nn.Embedding(input_vocab_size, d_model)\n        self.positional_encoding = PositionalEncoding(d_model, max_seq_len)\n        self.layers = nn.ModuleList([\n            EncoderLayer(d_model, num_heads, d_ff, dropout_rate)\n            for _ in range(num_layers)\n        ])\n        self.dropout = nn.Dropout(dropout_rate)\n\n    def forward(self, src: torch.Tensor, src_mask: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs a forward pass through the Encoder.\n\n        Args:\n            src (torch.Tensor): Source input sequence.\n                                Shape: [batch_size, seq_len_src].\n                                Example: [1, 10].\n            src_mask (torch.Tensor): Source mask to prevent attention to padding tokens.\n                                     Shape: [batch_size, 1, 1, seq_len_src].\n                                     Example: [1, 1, 1, 10].\n\n        Returns:\n            torch.Tensor: Output tensor of the encoder stack.\n                          Shape: [batch_size, seq_len_src, d_model].\n                          Example: [1, 10, 512].\n        \"\"\"\n        # [batch_size, seq_len_src] -> [batch_size, seq_len_src, d_model]\n        # Example: [1, 10] -> [1, 10, 512]\n        x = self.embedding(src) * math.sqrt(self.d_model) # Scale embeddings\n        x = self.dropout(self.positional_encoding(x))\n\n        for layer in self.layers:\n            # [batch_size, seq_len_src, d_model] -> [batch_size, seq_len_src, d_model]\n            # Example: [1, 10, 512] -> [1, 10, 512]\n            x = layer(x, src_mask)\n        return x\n\nclass DecoderLayer(nn.Module):\n    \"\"\"\n    Implements a single Decoder Layer of the Transformer.\n    Each layer consists of three sub-layers:\n    1. Masked Multi-Head Self-Attention Mechanism (to prevent attending to future positions).\n    2. Multi-Head Encoder-Decoder Attention Mechanism (queries from decoder, keys/values from encoder output).\n    3. Position-wise Fully Connected Feed-Forward Network.\n    Residual connections are employed around each sub-layer, followed by layer normalization.\n    \"\"\"\n    def __init__(self, d_model: int, num_heads: int, d_ff: int, dropout_rate: float = 0.1):\n        \"\"\"\n        Initializes a DecoderLayer.\n\n        Args:\n            d_model (int): The dimension of the model's embeddings (e.g., 512).\n            num_heads (int): The number of attention heads (e.g., 8).\n            d_ff (int): The dimension of the inner-layer of the feed-forward network (e.g., 2048).\n            dropout_rate (float): The dropout probability.\n        \"\"\"\n        super(DecoderLayer, self).__init__()\n        self.self_attn = MultiHeadAttention(d_model, num_heads, dropout_rate)\n        self.encoder_decoder_attn = MultiHeadAttention(d_model, num_heads, dropout_rate)\n        self.feed_forward = PositionwiseFeedForward(d_model, d_ff, dropout_rate)\n        self.norm1 = nn.LayerNorm(d_model)\n        self.norm2 = nn.LayerNorm(d_model)\n        self.norm3 = nn.LayerNorm(d_model)\n        self.dropout = nn.Dropout(dropout_rate)\n\n    def forward(self, x: torch.Tensor, enc_output: torch.Tensor,\n                src_mask: torch.Tensor, tgt_mask: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs a forward pass through a DecoderLayer.\n\n        Args:\n            x (torch.Tensor): Input tensor from the previous decoder layer.\n                              Shape: [batch_size, seq_len_tgt, d_model].\n                              Example: [1, 12, 512].\n            enc_output (torch.Tensor): Output tensor from the encoder stack.\n                                       Shape: [batch_size, seq_len_src, d_model].\n                                       Example: [1, 10, 512].\n            src_mask (torch.Tensor): Source mask for encoder-decoder attention.\n                                     Shape: [batch_size, 1, 1, seq_len_src].\n                                     Example: [1, 1, 1, 10].\n            tgt_mask (torch.Tensor): Target mask for masked self-attention in the decoder.\n                                     Shape: [batch_size, 1, seq_len_tgt, seq_len_tgt].\n                                     Example: [1, 1, 12, 12].\n\n        Returns:\n            torch.Tensor: Output tensor of the decoder layer.\n                          Shape: [batch_size, seq_len_tgt, d_model].\n                          Example: [1, 12, 512].\n        \"\"\"\n        # Sub-layer 1: Masked Multi-Head Self-Attention\n        # Queries, Keys, Values are all from the decoder's previous layer output (x)\n        # [batch_size, seq_len_tgt, d_model] -> [batch_size, seq_len_tgt, d_model]\n        # Example: [1, 12, 512] -> [1, 12, 512]\n        attn1_output = self.self_attn(x, x, x, tgt_mask)\n        x = self.norm1(x + self.dropout(attn1_output))\n\n        # Sub-layer 2: Multi-Head Encoder-Decoder Attention\n        # Queries from decoder's current output (x), Keys and Values from encoder's output\n        # [batch_size, seq_len_tgt, d_model] -> [batch_size, seq_len_tgt, d_model]\n        # Example: [1, 12, 512] -> [1, 12, 512]\n        attn2_output = self.encoder_decoder_attn(x, enc_output, enc_output, src_mask)\n        x = self.norm2(x + self.dropout(attn2_output))\n\n        # Sub-layer 3: Position-wise Feed-Forward\n        # [batch_size, seq_len_tgt, d_model] -> [batch_size, seq_len_tgt, d_model]\n        # Example: [1, 12, 512] -> [1, 12, 512]\n        ff_output = self.feed_forward(x)\n        x = self.norm3(x + self.dropout(ff_output))\n        return x\n\nclass Decoder(nn.Module):\n    \"\"\"\n    Implements the Decoder stack of the Transformer.\n    Composed of N identical DecoderLayers.\n    Output embeddings are combined with positional encodings.\n    \"\"\"\n    def __init__(self, target_vocab_size: int, d_model: int, num_layers: int,\n                 num_heads: int, d_ff: int, max_seq_len: int, dropout_rate: float = 0.1):\n        \"\"\"\n        Initializes the Decoder.\n\n        Args:\n            target_vocab_size (int): The size of the target vocabulary.\n            d_model (int): The dimension of the model's embeddings (e.g., 512).\n            num_layers (int): The number of identical decoder layers (N, e.g., 6).\n            num_heads (int): The number of attention heads (e.g., 8).\n            d_ff (int): The dimension of the inner-layer of the feed-forward network (e.g., 2048).\n            max_seq_len (int): The maximum sequence length for positional encoding.\n            dropout_rate (float): The dropout probability.\n        \"\"\"\n        super(Decoder, self).__init__()\n        self.d_model = d_model\n        self.embedding = nn.Embedding(target_vocab_size, d_model)\n        self.positional_encoding = PositionalEncoding(d_model, max_seq_len)\n        self.layers = nn.ModuleList([\n            DecoderLayer(d_model, num_heads, d_ff, dropout_rate)\n            for _ in range(num_layers)\n        ])\n        self.dropout = nn.Dropout(dropout_rate)\n\n    def forward(self, tgt: torch.Tensor, enc_output: torch.Tensor,\n                src_mask: torch.Tensor, tgt_mask: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs a forward pass through the Decoder.\n\n        Args:\n            tgt (torch.Tensor): Target input sequence (shifted right).\n                                Shape: [batch_size, seq_len_tgt].\n                                Example: [1, 12].\n            enc_output (torch.Tensor): Output tensor from the encoder stack.\n                                       Shape: [batch_size, seq_len_src, d_model].\n                                       Example: [1, 10, 512].\n            src_mask (torch.Tensor): Source mask for encoder-decoder attention.\n                                     Shape: [batch_size, 1, 1, seq_len_src].\n                                     Example: [1, 1, 1, 10].\n            tgt_mask (torch.Tensor): Target mask for masked self-attention in the decoder.\n                                     Shape: [batch_size, 1, seq_len_tgt, seq_len_tgt].\n                                     Example: [1, 1, 12, 12].\n\n        Returns:\n            torch.Tensor: Output tensor of the decoder stack.\n                          Shape: [batch_size, seq_len_tgt, d_model].\n                          Example: [1, 12, 512].\n        \"\"\"\n        # [batch_size, seq_len_tgt] -> [batch_size, seq_len_tgt, d_model]\n        # Example: [1, 12] -> [1, 12, 512]\n        x = self.embedding(tgt) * math.sqrt(self.d_model) # Scale embeddings\n        x = self.dropout(self.positional_encoding(x))\n\n        for layer in self.layers:\n            # [batch_size, seq_len_tgt, d_model] -> [batch_size, seq_len_tgt, d_model]\n            # Example: [1, 12, 512] -> [1, 12, 512]\n            x = layer(x, enc_output, src_mask, tgt_mask)\n        return x\n\nclass Transformer(nn.Module):\n    \"\"\"\n    Implements the Transformer model architecture as described in \"Attention Is All You Need\".\n    This model uses an encoder-decoder structure, relying solely on attention mechanisms.\n\n    Expected input shapes for a batch_size=1, src_seq_len=10, tgt_seq_len=12:\n    - src: [1, 10] (source sequence token IDs)\n    - tgt: [1, 12] (target sequence token IDs, shifted right)\n    - src_mask: [1, 1, 1, 10] (mask for source padding, 0 for padded positions)\n    - tgt_mask: [1, 1, 12, 12] (mask for target padding and subsequent positions, 0 for masked positions)\n    \"\"\"\n    def __init__(self, input_vocab_size: int, target_vocab_size: int, d_model: int = 512,\n                 num_layers: int = 6, num_heads: int = 8, d_ff: int = 2048,\n                 max_seq_len: int = 5000, dropout_rate: float = 0.1):\n        \"\"\"\n        Initializes the Transformer model.\n\n        Args:\n            input_vocab_size (int): The size of the input vocabulary.\n            target_vocab_size (int): The size of the target vocabulary.\n            d_model (int): The dimension of the model's embeddings and hidden states (e.g., 512).\n            num_layers (int): The number of identical encoder and decoder layers (N, e.g., 6).\n            num_heads (int): The number of attention heads (e.g., 8).\n            d_ff (int): The dimension of the inner-layer of the feed-forward network (e.g., 2048).\n            max_seq_len (int): The maximum sequence length for positional encoding.\n            dropout_rate (float): The dropout probability.\n        \"\"\"\n        super(Transformer, self).__init__()\n        self.encoder = Encoder(input_vocab_size, d_model, num_layers, num_heads, d_ff, max_seq_len, dropout_rate)\n        self.decoder = Decoder(target_vocab_size, d_model, num_layers, num_heads, d_ff, max_seq_len, dropout_rate)\n        self.generator = nn.Linear(d_model, target_vocab_size) # Final linear layer for output probabilities\n\n        # Share weights between target embedding and pre-softmax linear transformation\n        # and scale by sqrt(d_model) as per the paper.\n        self.generator.weight = self.decoder.embedding.weight\n        self.d_model = d_model\n\n    def forward(self, src: torch.Tensor, tgt: torch.Tensor,\n                src_mask: torch.Tensor, tgt_mask: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs a forward pass through the Transformer model.\n\n        Args:\n            src (torch.Tensor): Source input sequence.\n                                Shape: [batch_size, seq_len_src].\n                                Example: [1, 10].\n            tgt (torch.Tensor): Target input sequence (shifted right).\n                                Shape: [batch_size, seq_len_tgt].\n                                Example: [1, 12].\n            src_mask (torch.Tensor): Source mask to prevent attention to padding tokens.\n                                     Shape: [batch_size, 1, 1, seq_len_src].\n                                     Example: [1, 1, 1, 10].\n            tgt_mask (torch.Tensor): Target mask for masked self-attention in the decoder.\n                                     Shape: [batch_size, 1, seq_len_tgt, seq_len_tgt].\n                                     Example: [1, 1, 12, 12].\n\n        Returns:\n            torch.Tensor: Logits for the next token prediction.\n                          Shape: [batch_size, seq_len_tgt, target_vocab_size].\n                          Example: [1, 12, 10000].\n        \"\"\"\n        # [batch_size, seq_len_src] -> [batch_size, seq_len_src, d_model]\n        # Example: [1, 10] -> [1, 10, 512]\n        enc_output = self.encoder(src, src_mask)\n\n        # [batch_size, seq_len_tgt] -> [batch_size, seq_len_tgt, d_model]\n        # Example: [1, 12] -> [1, 12, 512]\n        dec_output = self.decoder(tgt, enc_output, src_mask, tgt_mask)\n\n        # [batch_size, seq_len_tgt, d_model] -> [batch_size, seq_len_tgt, target_vocab_size]\n        # Example: [1, 12, 512] -> [1, 12, 10000]\n        output = self.generator(dec_output)\n        return output\n\n\nif __name__ == \"__main__\":\n    # --- Hyperparameters and Vocabulary Sizes ---\n    input_vocab_size = 10000\n    target_vocab_size = 12000\n    d_model = 512\n    num_layers = 6\n    num_heads = 8\n    d_ff = 2048\n    max_seq_len = 200 # Max sequence length for positional encoding\n    dropout_rate = 0.1\n    pad_idx = 0 # Assuming 0 is the padding token ID\n\n    # --- Instantiate the Transformer model ---\n    model = Transformer(\n        input_vocab_size=input_vocab_size,\n        target_vocab_size=target_vocab_size,\n        d_model=d_model,\n        num_layers=num_layers,\n        num_heads=num_heads,\n        d_ff=d_ff,\n        max_seq_len=max_seq_len,\n        dropout_rate=dropout_rate\n    )\n\n    print(\"Model instantiated successfully!\")\n    print(f\"Number of parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad)}\")\n\n    # --- Create dummy input data ---\n    batch_size = 2\n    src_seq_len = 15\n    tgt_seq_len = 18\n\n    # Source sequence (e.g., English sentence token IDs)\n    # Example: [2, 5, 10, 3, 0, 0, ...] where 0 is padding\n    src = torch.randint(1, input_vocab_size, (batch_size, src_seq_len)) # Random tokens\n    src[:, src_seq_len - 3:] = pad_idx # Simulate some padding at the end\n\n    # Target sequence (e.g., German sentence token IDs, shifted right)\n    # Example: [1, 12, 20, 4, 0, 0, ...] where 1 is <SOS> and 0 is padding\n    tgt = torch.randint(1, target_vocab_size, (batch_size, tgt_seq_len)) # Random tokens\n    tgt[:, tgt_seq_len - 5:] = pad_idx # Simulate some padding at the end\n    tgt[:, 0] = 1 # Assuming 1 is <SOS> token\n\n    # --- Create masks ---\n    # Source mask: 1 for non-padding, 0 for padding. Shape: [batch_size, 1, 1, src_seq_len]\n    src_mask = (src != pad_idx).unsqueeze(1).unsqueeze(2).int()\n\n    # Target mask: combines padding mask and look-ahead mask.\n    # Padding mask: 1 for non-padding, 0 for padding. Shape: [batch_size, 1, tgt_seq_len]\n    tgt_padding_mask = (tgt != pad_idx).unsqueeze(1).int()\n\n    # Look-ahead mask: upper triangle (future positions) are 0, rest are 1.\n    # Shape: [1, 1, tgt_seq_len, tgt_seq_len]\n    look_ahead_mask = (1 - torch.triu(torch.ones(tgt_seq_len, tgt_seq_len), diagonal=1)).bool().int()\n    \n    # Combined target mask: 0 for padding OR future positions, 1 otherwise.\n    # Shape: [batch_size, 1, tgt_seq_len, tgt_seq_len]\n    tgt_mask = (tgt_padding_mask & look_ahead_mask.unsqueeze(0)).int()\n\n    print(f\"Input src shape: {src.shape}\") # Expected: [batch_size, src_seq_len]\n    print(f\"Input tgt shape: {tgt.shape}\") # Expected: [batch_size, tgt_seq_len]\n    print(f\"src_mask shape: {src_mask.shape}\") # Expected: [batch_size, 1, 1, src_seq_len]\n    print(f\"tgt_mask shape: {tgt_mask.shape}\") # Expected: [batch_size, 1, tgt_seq_len, tgt_seq_len]\n\n    # --- Run forward pass ---\n    print(\"Running forward pass...\")\n    output = model(src, tgt, src_mask, tgt_mask)\n\n    print(f\"Output shape: {output.shape}\") # Expected: [batch_size, tgt_seq_len, target_vocab_size]\n    assert output.shape == (batch_size, tgt_seq_len, target_vocab_size)\n    print(\"Forward pass successful! Output shape matches expectations.\")\n"
  }
}